import type { PracticeQuestion } from '../../types';

export const L22_PRACTICE: Record<string, PracticeQuestion[]> = {
  L22101: [
    {
      question: '在分析一組包含極端高收入者的薪資數據時，哪個集中趨勢量數最能代表「典型」的薪資水平？',
      options: {
        A: '平均數 (Mean)',
        B: '中位數 (Median)',
        C: '眾數 (Mode)',
        D: '全距 (Range)'
      },
      correctAnswer: 'B',
      explanation: '中位數對極端值（離群值）不敏感，因此在數據分佈不對稱或存在極端值時，它比平均數更能代表數據的中心趨勢。'
    },
    {
      question: '盒狀圖 (Box Plot) 主要用於展示數據的什麼資訊？',
      options: {
        A: '兩個變數之間的線性關係。',
        B: '數據隨時間的變化趨勢。',
        C: '單一變數的頻率分佈。',
        D: '中位數、四分位距以及離群值。'
      },
      correctAnswer: 'D',
      explanation: '盒狀圖能非常精簡地概括數據的分佈情況，包括中心位置（中位數）、離散程度（四分位距IQR）以及是否存在異常的離群值。'
    },
    {
        question: '一個數據集的標準差(Standard Deviation)很大，這代表什麼？',
        options: {
          A: '數據點都非常接近平均數。',
          B: '數據點的分佈非常分散。',
          C: '數據集的中位數大於平均數。',
          D: '數據集呈現對稱分佈。'
        },
        correctAnswer: 'B',
        explanation: '標準差是衡量數據離散程度的指標。標準差越大，表示數據點偏離其平均數的平均距離越大，即數據分佈越分散。'
    },
    {
        question: '如果一個數據分佈呈現右偏（正偏態），其平均數、中位數和眾數的關係通常是什麼？',
        options: {
          A: '平均數 < 中位數 < 眾數',
          B: '平均數 > 中位數 > 眾數',
          C: '平均數 = 中位數 = 眾數',
          D: '中位數 < 眾數 < 平均數'
        },
        correctAnswer: 'B',
        explanation: '在右偏分佈中，少數的極端高值會將平均數向右拉，因此平均數會大於中位數，而中位數通常會大於數據最集中的眾數。'
    },
    {
        question: '下列哪個圖表最適合用來觀察兩個連續數值變數（例如，身高和體重）之間的關係？',
        options: {
          A: '長條圖 (Bar Chart)',
          B: '直方圖 (Histogram)',
          C: '散佈圖 (Scatter Plot)',
          D: '圓餅圖 (Pie Chart)'
        },
        correctAnswer: 'C',
        explanation: '散佈圖是專門用來展示兩個數值變數之間關係的圖表，可以從中觀察到相關趨勢，如線性、非線性、正相關或負相關。'
    },
    {
        question: '哪個離散趨勢量數對離群值最不敏感（最穩健）？',
        options: {
          A: '全距 (Range)',
          B: '變異數 (Variance)',
          C: '標準差 (Standard Deviation)',
          D: '四分位距 (Interquartile Range, IQR)'
        },
        correctAnswer: 'D',
        explanation: '四分位距(IQR)只考慮了數據中間50%的範圍（Q3-Q1），忽略了頭尾各25%的極端數據，因此它對離群值的影響具有很強的抵抗力。'
    },
    {
        question: '某次考試的成績分布，若偏態係數(Skewness)顯著小於0，這通常意味著什麼？',
        options: {
          A: '考試太難，大部分學生分數偏低。',
          B: '考試太簡單，大部分學生分數偏高。',
          C: '學生成績呈現兩極分化。',
          D: '所有學生的分數都一樣。'
        },
        correctAnswer: 'B',
        explanation: '偏態係數小於0代表負偏態（左偏），分佈有一個長的左尾。在成績分布中，這意味著高分段學生非常集中，而少數低分學生拉長了左邊的尾巴，通常表示考試題目偏易。'
    },
    {
        question: '第90百分位數(90th percentile)的定義是什麼？',
        options: {
          A: '數據集中90%的數值都大於這個值。',
          B: '數據集中有90%的數值都小於或等於這個值。',
          C: '這個數值是數據集平均數的90%。',
          D: '數據集中最大的前10%數值的平均。'
        },
        correctAnswer: 'B',
        explanation: '百分位數是用來描述數據在分佈中相對位置的指標。第P百分位數指的是一個數值，使得數據集中有P%的觀測值小於或等於它。'
    },
    {
        question: '長條圖(Bar Chart)和直方圖(Histogram)最主要的區別是什麼？',
        options: {
          A: '長條圖是垂直的，直方圖是水平的。',
          B: '長條圖用於比較類別數據，其長條之間有間隙；直方圖用於展示數值數據的分佈，其長條之間通常沒有間隙。',
          C: '長條圖的高度代表頻率，直方圖的高度代表平均值。',
          D: '兩者沒有區別，可以互換使用。'
        },
        correctAnswer: 'B',
        explanation: '這是兩者最根本的區別。長條圖的橫軸是離散的類別，而直方圖的橫軸是連續的數值區間。'
    },
    {
        question: '下列何者是使用「全距(Range)」作為離散度量的主要缺點？',
        options: {
          A: '計算過程非常複雜。',
          B: '它只考慮了最大值和最小值，極易受到離群值的影響。',
          C: '它的單位與原始數據不同。',
          D: '它無法用於數值型數據。'
        },
        correctAnswer: 'B',
        explanation: '全距(Range = Max - Min)的計算只涉及兩個數據點，完全忽略了數據中間的分佈情況，因此它是一個非常不穩定且不穩健的離散度量。'
    }
  ],
  L22102: [
    {
      question: '某城市的數據顯示，平均每小時有10起交通事故，且事故發生是獨立的。若要對一小時內發生的事故次數進行建模，最適合使用哪種機率分佈？',
      options: {
        A: '常態分佈 (Normal Distribution)',
        B: '二項分佈 (Binomial Distribution)',
        C: '泊松分佈 (Poisson Distribution)',
        D: '均勻分佈 (Uniform Distribution)'
      },
      correctAnswer: 'C',
      explanation: '泊松分佈專門用來描述在一個固定的時間間隔或空間區域內，某個獨立隨機事件發生的次數。'
    },
    {
      question: '中央極限定理 (CLT) 的核心結論是什麼？',
      options: {
        A: '任何數據集最終都會呈現常態分佈。',
        B: '只要樣本量足夠大，樣本平均數的抽樣分佈會近似於常態分佈。',
        C: '只有來自常態分佈母體的樣本，其平均數才會是常態分佈。',
        D: '樣本量越大，樣本的變異數越小。'
      },
      correctAnswer: 'B',
      explanation: 'CLT的強大之處在於，不論原始母體的分佈如何，只要樣本量足夠大，其樣本均值的抽樣分佈就會趨近於常態分佈。這是統計推斷的理論基石。'
    },
    {
        question: '下列哪個分佈是連續機率分佈？',
        options: {
          A: '伯努利分佈 (Bernoulli)',
          B: '二項分佈 (Binomial)',
          C: '泊松分佈 (Poisson)',
          D: '常態分佈 (Normal)'
        },
        correctAnswer: 'D',
        explanation: '常態分佈的隨機變數可以在一個區間內取任何連續的值，是連續機率分佈的代表。A、B、C都是描述計數次數的離散機率分佈。'
    },
    {
        question: '一個呼叫中心的來電次數服從泊松分佈，那麼兩次來電之間的時間間隔，最適合用哪個分佈來建模？',
        options: {
          A: '指數分佈 (Exponential)',
          B: '常態分佈 (Normal)',
          C: '均勻分佈 (Uniform)',
          D: '伯努利分佈 (Bernoulli)'
        },
        correctAnswer: 'A',
        explanation: '指數分佈與泊松分佈密切相關，它專門用來描述獨立隨機事件發生的時間間隔。'
    },
    {
        question: '中央極限定理(CLT)為何如此重要？',
        options: {
          A: '它證明了所有數據都是常態分佈。',
          B: '它為我們能夠用樣本來估計母體參數（如建立信賴區間）提供了理論保證。',
          C: '它簡化了所有機器學習演算法的計算。',
          D: '它只適用於小樣本數據。'
        },
        correctAnswer: 'B',
        explanation: '因為CLT告訴我們樣本平均數的分佈規律（近似常態），所以我們才能夠基於樣本的統計量，對未知的母體參數進行科學的推斷和假設檢定。'
    },
    {
        question: '在A/B測試中，我們觀察一個用戶是否點擊廣告（只有「點擊」或「未點擊」兩種結果）。這個單一用戶的行為可以用哪個分佈來描述？',
        options: {
          A: '泊松分佈',
          B: '常態分佈',
          C: '伯努利分佈',
          D: '二項分佈'
        },
        correctAnswer: 'C',
        explanation: '伯努利分佈是描述單次試驗、只有兩種可能結果的機率分佈，完美符合這個場景。'
    },
    {
        question: '接續上題，如果我們觀察了1000個用戶，並計算總共有多少用戶點擊了廣告，這個「總點擊次數」可以用哪個分佈來描述？',
        options: {
          A: '泊松分佈',
          B: '常態分佈',
          C: '伯努利分佈',
          D: '二項分佈'
        },
        correctAnswer: 'D',
        explanation: '二項分佈描述的是在n次獨立的伯努利試驗中，「成功」結果出現的總次數。'
    },
    {
        question: '常態分佈由哪兩個參數完全決定？',
        options: {
          A: '均值和中位數',
          B: '均值和標準差',
          C: '最小值和最大值',
          D: '偏態和峰態'
        },
        correctAnswer: 'B',
        explanation: '常態分佈的機率密度函數僅由均值(μ)和標準差(σ)兩個參數確定。均值決定了分佈的中心位置，標準差決定了分佈的胖瘦程度。'
    },
    {
        question: '中央極限定理的一個重要前提條件是什麼？',
        options: {
          A: '母體必須是常態分佈。',
          B: '樣本必須是隨機且獨立抽取的，且樣本量要足夠大。',
          C: '樣本量必須很小（n<10）。',
          D: '抽樣必須是有偏的。'
        },
        correctAnswer: 'B',
        explanation: 'CLT的結論成立，需要滿足樣本的隨機性、獨立性以及足夠大的樣本量（通常n≥30）這幾個關鍵條件。'
    },
    {
        question: '在金融領域，為何有時不能盲目假設股價報酬服從常態分佈？',
        options: {
          A: '因為股價報酬都是正數。',
          B: '因為股價報酬的分佈通常是「肥尾」的，極端事件（股災）的發生機率遠高於常態分佈的預測。',
          C: '因為常態分佈的計算太複雜。',
          D: '因為股價報酬是離散的。'
        },
        correctAnswer: 'B',
        explanation: '許多真實世界的金融數據呈現「高峰態」和「肥尾」特性，這意味著極端值的出現比常態分佈預期的更頻繁。盲目套用常態分佈會嚴重低估風險。'
    }
  ],
  L22103: [
    {
      question: '在假設檢定中，P值的正確定義是什麼？',
      options: {
        A: '虛無假設(H0)為真的機率。',
        B: '對立假設(H1)為真的機率。',
        C: '在假定H0為真的前提下，觀測到當前或更極端結果的機率。',
        D: '實驗結果出錯的機率。'
      },
      correctAnswer: 'C',
      explanation: 'P值衡量的是樣本證據與虛無假設之間的不一致程度。P值越小，表示我們的觀測結果在H0為真的情況下越不可能發生，因此我們越有理由拒絕H0。'
    },
    {
      question: '在進行A/B測試時，若錯誤地拒絕了虛無假設（即新舊版本無差異），而宣稱新版本效果更好，這屬於哪種類型的錯誤？',
      options: {
        A: '第一類型錯誤 (Type I Error)',
        B: '第二類型錯誤 (Type II Error)',
        C: '隨機錯誤',
        D: '系統錯誤'
      },
      correctAnswer: 'A',
      explanation: '第一類型錯誤（棄真錯誤）指的是虛無假設實際上為真，但我們卻錯誤地拒絕了它。其發生的機率由我們預設的顯著水準α控制。'
    },
    {
        question: '在假設檢定中，虛無假設(H0)通常是一個什麼樣的陳述？',
        options: {
          A: '研究者希望證明的效果。',
          B: '一個「基準」、「無效果」或「無差異」的陳述。',
          C: '一個永遠為真的陳述。',
          D: '一個永遠為假的陳述。'
        },
        correctAnswer: 'B',
        explanation: '虛無假設代表的是現狀或我們想要推翻的基準觀點，如同法庭上的「無罪推定」，檢定過程就是試圖找到足夠的證據來反駁它。'
    },
    {
        question: '如果一個假設檢定的P值是0.03，而我們預設的顯著水準α是0.05，我們應該做出什麼決策？',
        options: {
          A: '接受虛無假設(H0)。',
          B: '無法拒絕虛無假設(H0)。',
          C: '拒絕虛無假設(H0)。',
          D: '增加樣本量再重新檢定。'
        },
        correctAnswer: 'C',
        explanation: '決策規則是：如果 P值 < α，我們就拒絕虛無假設。因為0.03 < 0.05，所以我們有足夠的統計證據來拒絕H0。'
    },
    {
        question: '一個新藥的臨床試驗中，第二類型錯誤(Type II Error)指的是什麼？',
        options: {
          A: '藥物其實無效，但實驗結論說它有效。',
          B: '藥物其實是有效的，但實驗未能發現其效果。',
          C: '實驗過程中發生了操作失誤。',
          D: '病人謊報了自己的症狀。'
        },
        correctAnswer: 'B',
        explanation: '第二類型錯誤（取偽錯誤）指的是虛無假設實際上為偽（即藥物有效），但我們卻未能找到足夠的證據來拒絕它，導致錯過了一個有效的藥物。'
    },
    {
        question: '統計檢定力(Statistical Power)的定義是什麼？',
        options: {
          A: '犯第一類型錯誤的機率。',
          B: '犯第二類型錯誤的機率。',
          C: '當對立假設為真時，我們能正確地拒絕虛無假設的機率。',
          D: '樣本數據的平均值。'
        },
        correctAnswer: 'C',
        explanation: '檢定力是衡量一個實驗設計好壞的重要指標，它代表了實驗「偵測」到真實效果的能力。檢定力等於 1 - β (第二類型錯誤的機率)。'
    },
    {
        question: '如果一個研究的結果是「無法拒絕虛無假設」，這是否意味著「證明了虛無假設為真」？',
        options: {
          A: '是，兩者是相同的意思。',
          B: '否，它只表示我們當前的證據不足以推翻虛無假設。',
          C: '是，並且可以推廣到所有情況。',
          D: '否，這意味著實驗完全失敗了。'
        },
        correctAnswer: 'B',
        explanation: '這是對假設檢定結論最常見的誤解之一。「證據的缺席不等於缺席的證據」。未能拒絕H0是一個中性的結論，而非接受H0。'
    },
    {
        question: '在進行多重比較（例如，同時測試20種不同的廣告文案）時，可能會出現什麼問題？',
        options: {
          A: '檢定力會大幅提升。',
          B: '犯第一類型錯誤（假陽性）的機率會顯著增加。',
          C: '所有P值都會變大。',
          D: '計算成本會降低。'
        },
        correctAnswer: 'B',
        explanation: '如果每次檢定的α是0.05，那麼進行20次獨立檢定，至少有一次出現假陽性的機率會遠高於5%。這就是多重比較問題，需要進行P值校正。'
    },
    {
        question: '在一個樣本量極大的A/B測試中，我們發現新版本的點擊率比舊版本高了0.001%，且P值小於0.001。這個結果是？',
        options: {
          A: '統計上不顯著，但實際上顯著。',
          B: '統計上顯著，且實際上也顯著。',
          C: '統計上顯著，但實際上可能不顯著。',
          D: '統計上和實際上都不顯著。'
        },
        correctAnswer: 'C',
        explanation: 'P值很小，代表這個差異具有「統計顯著性」。但0.001%的提升在商業上可能毫無意義，即不具備「實際顯著性」。這凸顯了在大數據時代，不能只看P值，必須結合效應量來做決策。'
    },
    {
        question: '對立假設(H1)在假設檢定中扮演什麼角色？',
        options: {
          A: '它總是代表「無效果」。',
          B: '它是我們想要用證據來推翻的假設。',
          C: '它是研究者真正感興趣、希望找到證據來支持的陳述。',
          D: '它總是與虛無假設(H0)相同。'
        },
        correctAnswer: 'C',
        explanation: '對立假設（或稱研究假設）代表了我們預期的、想要證明的效果。整個檢定的過程就是試圖收集足夠的證據來拒絕H0，從而間接支持H1。'
    }
  ],
  L22201: [
    {
      question: '在數據清理過程中，面對數據中的缺失值，下列哪種處理方法最為簡單快速，但可能扭曲數據分佈？',
      options: {
        A: '用特徵的中位數或平均數插補。',
        B: '直接刪除含有缺失值的整行數據。',
        C: '使用K-NN模型預測並填補缺失值。',
        D: '將「缺失」本身作為一個新的特徵。'
      },
      correctAnswer: 'A',
      explanation: '使用均值、中位數或眾數進行插補是最簡單的方法，但它會降低數據的變異性，可能影響後續分析的準確性。'
    },
    {
        question: '從網站伺服器、應用程式或IoT設備產生的行為日誌，屬於哪一類數據收集來源？',
        options: {
            A: 'API (應用程式介面)',
            B: '網路爬蟲 (Web Scraping)',
            C: '資料庫查詢',
            D: '日誌文件 (Log Files)'
        },
        correctAnswer: 'D',
        explanation: '日誌文件是系統或應用在運行時自動生成的記錄，包含了詳細的時間戳和事件資訊，是分析用戶行為和診斷系統問題的重要數據源。'
    },
    {
        question: '使用Python的BeautifulSoup或Scrapy框架，自動化地瀏覽網頁並抓取公開資訊，這種數據收集方法稱為什麼？',
        options: {
          A: 'API調用',
          B: '網路爬蟲 (Web Scraping)',
          C: '資料庫查詢',
          D: '串流數據'
        },
        correctAnswer: 'B',
        explanation: '網路爬蟲是一種從非結構化的HTML網頁中提取數據的技術，靈活性高，但需要處理反爬機制和頁面變動的問題。'
    },
    {
        question: '將「USA」、「United States」、「America」都統一為「USA」，這屬於數據清理的哪個環節？',
        options: {
          A: '處理缺失值',
          B: '處理離群值',
          C: '處理格式不一致',
          D: '移除重複數據'
        },
        correctAnswer: 'C',
        explanation: '這是典型的處理類別名稱不一致的問題，目的是將代表相同意義的不同寫法標準化，以確保數據的一致性。'
    },
    {
        question: '在處理離群值時，為何不能輕易地將其刪除？',
        options: {
          A: '因為刪除數據會減少數據集的總量。',
          B: '因為離群值可能是真實但極端的罕見事件，本身可能就是分析的重點。',
          C: '因為刪除數據是違法的。',
          D: '因為離群值對模型沒有任何影響。'
        },
        correctAnswer: 'B',
        explanation: '處理離群值必須非常謹慎。一個離群值可能是輸入錯誤，但也可能是一次真實的詐欺交易或一位超級VIP客戶，這些罕見事件往往蘊含著重要的商業價值或風險信號。'
    },
    {
        question: '在房價預測數據中，發現一筆交易的屋齡是-5年，這屬於什麼樣的數據問題？',
        options: {
          A: '數據格式不一致',
          B: '數據不滿足有效性規則',
          C: '數據缺失',
          D: '數據重複'
        },
        correctAnswer: 'B',
        explanation: '屋齡不應該是負數，這違反了數據的業務邏輯或有效性規則。這類數據通常是輸入錯誤造成的，需要進行修正或移除。'
    },
    {
        question: '有時數據的「缺失」本身就是一種有用的資訊。此時，最佳的處理策略是什麼？',
        options: {
          A: '直接刪除該行數據。',
          B: '用平均值填補。',
          C: '將其作為一個新的二元特徵（例如，「是否缺失」），然後再對原始欄位進行插補。',
          D: '用0來填補。'
        },
        correctAnswer: 'C',
        explanation: '這種方法可以同時保留「缺失」這個行為所帶來的資訊，又能讓原始欄位被填充以供模型使用，是一種更全面的處理策略。'
    },
    {
        question: '進行對數轉換(log transformation)通常是為了解決什麼數據問題？',
        options: {
          A: '數據缺失',
          B: '數據重複',
          C: '數據格式不一致',
          D: '由正偏態分佈引起的離群值問題'
        },
        correctAnswer: 'D',
        explanation: '對數轉換可以有效地壓縮數據的尺度，將一個嚴重右偏的數據（如收入分佈）轉換為一個更接近對稱的分佈，從而緩解極端高值的影響。'
    },
    {
        question: '下列何者是數據清理步驟中最耗時、最核心的挑戰？',
        options: {
          A: '數據清理通常佔據整個數據專案60%-80%的時間和精力。',
          B: '數據清理的步驟有標準答案，不需要主觀判斷。',
          C: '數據清理可以完全被自動化工具取代。',
          D: '數據清理的順序不重要。'
        },
        correctAnswer: 'A',
        explanation: '數據清理是一個高度迭代且勞力密集的工作。由於真實世界數據的混亂程度遠超預期，分析師和工程師需要花費大量的時間來理解、清洗和準備數據。'
    },
    {
        question: '下列哪個數據收集方法最穩定、最可靠？',
        options: {
          A: '網路爬蟲',
          B: 'API (應用程式介面)',
          C: '人工輸入',
          D: '掃描紙本文件'
        },
        correctAnswer: 'B',
        explanation: 'API是由服務提供商官方提供的、結構化的數據接口。相比於隨時可能變動的網頁爬蟲，API的數據格式和獲取方式都更穩定、更可靠。'
    }
  ],
  L22202: [
    {
        question: '下列哪種資料庫技術最適合儲存和查詢複雜的網絡關係，例如社群網路中的「好友的好友」？',
        options: {
            A: '鍵值儲存 (Key-Value)',
            B: '文件儲存 (Document)',
            C: '關聯式資料庫 (RDBMS)',
            D: '圖形資料庫 (Graph)'
        },
        correctAnswer: 'D',
        explanation: '圖形資料庫（如Neo4j）專為儲存節點和邊（關係）而設計，對於遍歷和查詢複雜的、多層次的網絡關係，其性能遠超其他類型的資料庫。'
    },
    {
        question: '「讀取時綱要 (Schema-on-Read)」是下列哪種數據儲存架構的核心特性？',
        options: {
            A: '關聯式資料庫 (RDBMS)',
            B: '數據倉儲 (Data Warehouse)',
            C: '數據湖 (Data Lake)',
            D: '線上交易處理系統 (OLTP)'
        },
        correctAnswer: 'C',
        explanation: '數據湖允許以原始格式儲存任何類型的數據，在進行分析（讀取）時才定義其結構。這與RDBMS和數據倉儲的「寫入時綱要」形成對比，為數據科學家提供了極大的靈活性。'
    },
    {
        question: '對數據一致性要求極高的線上交易處理系統(OLTP)，如銀行帳務系統，最適合使用哪種資料庫？',
        options: {
          A: '關聯式資料庫 (RDBMS)',
          B: '文件儲存資料庫',
          C: '鍵值儲存資料庫',
          D: '圖形資料庫'
        },
        correctAnswer: 'A',
        explanation: '關聯式資料庫遵循ACID原則，為數據交易提供了強一致性的保證，這是金融等關鍵業務系統不可或缺的特性。'
    },
    {
        question: '數據倉儲(Data Warehouse)的主要目的是什麼？',
        options: {
          A: '處理高併發的即時交易。',
          B: '儲存非結構化的原始數據以供數據科學家探索。',
          C: '整合、清洗來自多個營運系統的歷史數據，以支持商業智慧(BI)和分析型查詢(OLAP)。',
          D: '作為網站的快取層以提升速度。'
        },
        correctAnswer: 'C',
        explanation: '數據倉儲是一個專為分析而設計的中央數據庫，它將來自不同系統的數據進行整合與轉換，以主題導向的方式組織起來，服務於報表和決策支持。'
    },
    {
        question: 'MongoDB是一種流行的NoSQL資料庫，它屬於哪一種類型？',
        options: {
          A: '鍵值儲存',
          B: '文件儲存',
          C: '列式儲存',
          D: '圖形資料庫'
        },
        correctAnswer: 'B',
        explanation: 'MongoDB以類JSON格式(BSON)的文檔來儲存數據，每個文檔的結構可以不同，這種靈活性使其非常適合儲存半結構化數據。'
    },
    {
        question: '如果數據湖缺乏有效的數據治理，它最可能變成什麼？',
        options: {
          A: '數據倉儲',
          B: '數據中心',
          C: '數據沼澤 (Data Swamp)',
          D: '數據庫'
        },
        correctAnswer: 'C',
        explanation: '數據沼澤指的是一個管理混亂、元數據缺失、數據品質低劣的數據湖，其中的數據難以被發現、理解和信任，最終失去了其價值。'
    },
    {
        question: 'NoSQL資料庫通常採用哪種方式來進行擴展？',
        options: {
          A: '垂直擴展 (Scale-up)',
          B: '水平擴展 (Scale-out)',
          C: '不支援擴展',
          D: '僅支援縮小'
        },
        correctAnswer: 'B',
        explanation: '水平擴展（即增加更多普通伺服器到叢集中）是NoSQL資料庫的核心設計理念之一，這使得它們能夠以較低的成本處理海量的數據和流量。'
    },
    {
        question: '關聯式資料庫和數據倉儲的共同點是什麼？',
        options: {
          A: '它們都主要用於線上交易處理(OLTP)。',
          B: '它們都採用「讀取時綱要」。',
          C: '它們都主要儲存結構化數據，並採用「寫入時綱要」。',
          D: '它們都主要儲存非結構化數據。'
        },
        correctAnswer: 'C',
        explanation: '數據倉儲雖然是為分析而設計，但其底層仍然是基於結構化的表格和預定義的綱要，這一點與傳統的關聯式資料庫是相似的。'
    },
    {
        question: 'Redis這種資料庫主要被用作什麼？',
        options: {
          A: '大規模數據分析',
          B: '複雜的關係查詢',
          C: '快取層和Session管理',
          D: '長期歷史數據歸檔'
        },
        correctAnswer: 'C',
        explanation: 'Redis是一種基於記憶體的鍵值儲存資料庫，其讀寫速度極快，非常適合用作應用程式的快取層，以減輕後端資料庫的壓力。'
    },
    {
        question: 'Lakehouse架構試圖結合哪兩種技術的優點？',
        options: {
          A: '關聯式資料庫和NoSQL資料庫',
          B: '數據湖和數據倉儲',
          C: '鍵值儲存和圖形資料庫',
          D: '本地儲存和雲端儲存'
        },
        correctAnswer: 'B',
        explanation: 'Lakehouse是一個新興的數據架構，它試圖在數據湖（靈活性、支持多種數據類型）的基礎上，引入數據倉儲的ACID交易、數據管理和查詢性能等優點。'
    }
  ],
  L22203: [
    {
        question: '相較於Hadoop MapReduce，Apache Spark最主要的性能優勢來自於？',
        options: {
            A: '使用Java語言編寫。',
            B: '基於記憶體的運算 (In-Memory Computing)。',
            C: '更好的磁碟讀寫效率。',
            D: '與HDFS的整合更緊密。'
        },
        correctAnswer: 'B',
        explanation: 'Spark將中間計算結果盡可能地保留在記憶體中，避免了MapReduce在每一步計算後都需要讀寫磁碟的瓶頸，因此在迭代式計算和交互式查詢場景下，速度可以快上百倍。'
    },
    {
      question: 'MapReduce編程模型的核心思想是什麼？',
      options: {
          A: '將所有數據載入單一伺服器的記憶體中進行處理。',
          B: '將大型計算任務「分而治之」，透過Map和Reduce兩個階段進行平行處理。',
          C: '專門為即時串流數據處理設計。',
          D: '一種用於查詢關聯式資料庫的語言。'
      },
      correctAnswer: 'B',
      explanation: 'MapReduce的核心是「分而治之」。Map階段負責將任務分解並行化，Reduce階段負責將並行處理的結果進行匯總，從而實現對大規模數據集的處理。'
    },
    {
        question: '在Spark生態系統中，哪個元件專門用於處理結構化數據和執行SQL查詢？',
        options: {
            A: 'Spark Core',
            B: 'MLlib',
            C: 'Structured Streaming',
            D: 'Spark SQL'
        },
        correctAnswer: 'D',
        explanation: 'Spark SQL是Spark中用於處理結構化數據的模組，它允許開發者使用SQL語句或DataFrame API來進行高效的數據查詢和分析。'
    },
    {
        question: '在Spark中，相較於早期的RDD API，使用DataFrame/Dataset API的主要好處是什麼？',
        options: {
          A: '執行速度比較慢。',
          B: '只能處理非結構化數據。',
          C: '引入了綱要(Schema)和優化器(Catalyst)，能自動優化查詢計畫，執行效率更高。',
          D: '程式碼寫起來更複雜。'
        },
        correctAnswer: 'C',
        explanation: 'DataFrame/Dataset是更高層次的API，它們讓Spark能夠理解數據的結構，從而利用Catalyst優化器和Tungsten執行引擎來大幅優化查詢性能，通常比手寫的RDD操作效率更高。'
    },
    {
        question: '在分散式計算中，哪個操作通常是最昂貴、最耗時的？',
        options: {
          A: 'Map操作',
          B: 'Filter操作',
          C: 'Shuffle操作',
          D: 'Read操作'
        },
        correctAnswer: 'C',
        explanation: 'Shuffle操作需要在叢集的節點之間透過網路大量傳輸數據以進行重新分組和排序，這個過程涉及到大量的磁碟I/O和網路I/O，是分散式計算中最主要的性能瓶頸。'
    },
    {
        question: 'Spark MLlib是Spark生態系中的哪個元件？',
        options: {
          A: '用於圖計算的函式庫。',
          B: '提供常用機器學習演算法分散式實現的函式庫。',
          C: '用於串流數據處理的引擎。',
          D: 'Spark的核心排程器。'
        },
        correctAnswer: 'B',
        explanation: 'MLlib是Spark的機器學習函式庫，它提供了諸如分類、迴歸、聚類等常用演算法的分散式版本，讓數據科學家能夠在大型數據集上進行模型訓練。'
    },
    {
        question: '下列哪個場景最適合使用Spark的Structured Streaming？',
        options: {
          A: '對一個固定的歷史數據集進行一次性的ETL處理。',
          B: '對來自Kafka的即時交易數據流進行反詐欺偵測。',
          C: '訓練一個深度學習的圖像分類模型。',
          D: '執行一個複雜的SQL查詢。'
        },
        correctAnswer: 'B',
        explanation: 'Structured Streaming是Spark用於處理即時數據流的引擎，它能夠以高吞吐和可容錯的方式，對持續產生的數據進行近乎即時的分析。'
    },
    {
        question: '為何說MapReduce的思想是所有現代分散式計算框架的基礎？',
        options: {
          A: '因為所有框架都還在直接使用MapReduce的程式碼。',
          B: '因為「分而治之」以及Shuffle & Sort這兩個核心概念，仍然是Spark等框架底層運作的基礎。',
          C: '因為MapReduce的發明人也發明了所有其他框架。',
          D: '因為MapReduce是唯一開源的框架。'
        },
        correctAnswer: 'B',
        explanation: '儘管Spark等新框架在執行效率上遠超MapReduce，但其底層的計算模型仍然遵循著MapReduce所開創的「分片-映射-洗牌-化約」這一基本典範。'
    },
    {
        question: '處理海量小檔案是HDFS等分散式檔案系統的一個痛點，其主要原因是什麼？',
        options: {
          A: '小檔案會佔用更多的磁碟空間。',
          B: '每個檔案的元數據(metadata)都需要在NameNode中佔用記憶體，檔案過多會耗盡NameNode的記憶體。',
          C: '小檔案無法被壓縮。',
          D: '讀取小檔案需要更多的網路頻寬。'
        },
        correctAnswer: 'B',
        explanation: 'HDFS的架構中，NameNode負責管理整個檔案系統的元數據，而這些元數據是儲存在記憶體中的。大量的小檔案會產生大量的元數據，可能導致NameNode記憶體不足，成為整個叢集的瓶頸。'
    },
    {
        question: '下列何者不是Spark相比於MapReduce的優勢？',
        options: {
          A: '基於記憶體的計算，速度更快。',
          B: '提供了統一的、涵蓋多種處理場景的生態系統。',
          C: '擁有更易於使用的DataFrame/SQL高層次API。',
          D: '硬體成本更低廉。'
        },
        correctAnswer: 'D',
        explanation: 'Spark和MapReduce通常都運行在由普通商用伺服器組成的叢集上，兩者在硬體成本上沒有本質區別。Spark的優勢主要體現在軟體層面的性能和易用性。'
    }
  ],
  L22301: [
    {
        question: '在大數據分析中，為何「實際顯著性 (Practical Significance)」與「統計顯著性 (Statistical Significance)」同等重要？',
        options: {
            A: '因為大樣本量會讓任何微小的差異都變得統計顯著，但這些差異可能沒有商業價值。',
            B: '因為兩者是完全相同的概念。',
            C: '因為實際顯著性決定了P值的大小。',
            D: '因為在大數據中無法計算統計顯著性。'
        },
        correctAnswer: 'A',
        explanation: '大數據會放大統計顯著性，導致即使是非常微小的、在商業上無意義的效果（低實際顯著性），其P值也可能很小。因此必須結合效應量(Effect Size)來判斷結果是否真的重要。'
    },
    {
        question: 'A/B測試能夠進行可靠的因果推斷，其最關鍵的步驟是什麼？',
        options: {
            A: '確保實驗組的樣本量遠大於控制組。',
            B: '對用戶進行隨機分流。',
            C: '在實驗後對用戶進行問卷調查。',
            D: '選擇一個非常小的P值門檻。'
        },
        correctAnswer: 'B',
        explanation: '隨機化是A/B測試的靈魂。透過將用戶隨機分配到不同組別，可以確保兩組用戶在所有其他潛在的混淆變數上是統計上相似的，從而將觀測到的差異歸因於我們所做的唯一改變。'
    },
    {
        question: '在對一個包含不同用戶等級（如免費、付費、VIP）的數據集進行抽樣時，為了確保樣本中各等級用戶的比例與母體一致，應採用哪種抽樣方法？',
        options: {
            A: '簡單隨機抽樣',
            B: '系統抽樣',
            C: '分層抽樣',
            D: '叢集抽樣'
        },
        correctAnswer: 'C',
        explanation: '分層抽樣先將母體按特定變數分層，再從每層中按比例抽樣。這能確保樣本的結構與母體一致，對於提高推斷的準確性非常重要。'
    },
    {
        question: '辛普森悖論(Simpson\'s Paradox)描述的是什麼現象？',
        options: {
          A: '當數據量太小時，統計結果不可靠。',
          B: '當忽略了某個重要的混淆變數時，分組和匯總後的數據可能呈現完全相反的趨勢。',
          C: '兩個不相關的變數可能呈現出虛假的相關性。',
          D: 'A/B測試的結果總是與直覺相反。'
        },
        correctAnswer: 'B',
        explanation: '辛普森悖論是一個統計學上的陷阱，它提醒我們在分析數據時，必須警惕潛在的混淆變數，否則匯總數據的結論可能是完全錯誤的。'
    },
    {
        question: '為何即使在擁有海量數據的情況下，抽樣依然很重要？',
        options: {
          A: '因為抽樣可以增加數據的總量。',
          B: '因為直接在全部數據上進行複雜分析的成本可能過高，透過抽樣可以快速進行探索和推斷。',
          C: '因為法律規定必須進行抽樣。',
          D: '因為抽樣後的數據準確性更高。'
        },
        correctAnswer: 'B',
        explanation: '抽樣在效率和成本上具有巨大優勢。在很多情況下，一個有代表性的樣本已經足以得出可靠的結論，而無需花費巨大的計算資源去處理全部數據。'
    },
    {
        question: '下列哪個抽樣方法，操作最簡單，但如果數據存在週期性，可能產生偏差？',
        options: {
          A: '簡單隨機抽樣',
          B: '分層抽樣',
          C: '系統抽樣',
          D: '叢集抽樣'
        },
        correctAnswer: 'C',
        explanation: '系統抽樣（每隔k個抽一個）雖然簡單，但如果數據的排序本身就存在與k相關的週期性，抽出的樣本就會有嚴重的偏誤。'
    },
    {
        question: '在A/B測試中，「隨機分流」的主要目的是什麼？',
        options: {
          A: '讓實驗變得更複雜。',
          B: '盡可能地消除或平衡其他混淆變數的影響。',
          C: '確保兩組的樣本量完全相等。',
          D: '讓點擊率更高的一組獲得更多流量。'
        },
        correctAnswer: 'B',
        explanation: '隨機化的力量在於，只要樣本量足夠大，它就能在統計上確保A組和B組在所有我們已知和未知的特徵上（如用戶年齡、地理位置、活躍度）都是相似的，從而隔離出我們想要測試的唯一變數的效果。'
    },
    {
        question: '衡量「實際顯著性」的指標是什麼？',
        options: {
          A: 'P值 (P-value)',
          B: '樣本量 (Sample Size)',
          C: '效應量 (Effect Size)',
          D: '信賴水準 (Confidence Level)'
        },
        correctAnswer: 'C',
        explanation: '效應量是衡量差異或關係強度的量化指標（例如，點擊率提升了多少個百分點）。它回答了「效果有多大？」這個問題，與P值回答的「效果是否為偶然？」是互補的。'
    },
    {
        question: '下列哪個情境最不適合使用A/B測試？',
        options: {
          A: '測試網站按鈕顏色對點擊率的影響。',
          B: '測試兩種不同推薦演算法對用戶留存率的影響。',
          C: '測試新的產品定價策略對營收的影響。',
          D: '分析過去十年的銷售數據，找出季節性趨勢。'
        },
        correctAnswer: 'D',
        explanation: 'A/B測試是一種「前瞻性」的實驗方法，用於比較不同方案的效果。而分析歷史數據以找出趨勢，是一種「回溯性」的觀察性分析，不屬於A/B測試。'
    },
    {
        question: '在大數據時代，分析師為何需要更加警惕「統計顯著性」的陷阱？',
        options: {
          A: '因為大數據的P值很難計算。',
          B: '因為大數據會讓P值變得不穩定。',
          C: '因為巨大的樣本量會讓任何微不足道的差異都變得統計顯著。',
          D: '因為大數據中不存在任何隨機性。'
        },
        correctAnswer: 'C',
        explanation: 'P值的大小與樣本量成反比。在數百萬甚至數億的樣本量下，幾乎任何微小的、無關緊要的差異都會得到一個極小的P值。因此，此時更應該關注效應量的大小。'
    }
  ],
  L22302: [
    {
        question: '「購物籃分析」中最常用來發現「啤酒與尿布」這類商品關聯性的分析方法是？',
        options: {
            A: '聚類分析 (Clustering)',
            B: '分類分析 (Classification)',
            C: '迴歸分析 (Regression)',
            D: '關聯規則分析 (Association Rule Mining)'
        },
        correctAnswer: 'D',
        explanation: '關聯規則分析專門用於從交易數據中發現項目之間有趣的併發關係，其核心指標包括支持度、信賴度和提升度。'
    },
    {
        question: '下列哪個分析方法屬於「非監督式學習」？',
        options: {
            A: '線性迴歸',
            B: 'K-均值聚類 (K-Means)',
            C: '隨機森林',
            D: '邏輯迴歸'
        },
        correctAnswer: 'B',
        explanation: 'K-均值聚類是一種非監督式學習演算法，因為它處理的是沒有預先標籤的數據，其目標是自動發現數據中的群體結構。其他選項都是監督式學習演算法。'
    },
    {
        question: '預測一位客戶未來的消費金額，這個問題屬於哪一類大數據分析方法？',
        options: {
            A: '聚類分析',
            B: '分類分析',
            C: '迴歸分析',
            D: '關聯規則分析'
        },
        correctAnswer: 'C',
        explanation: '由於「消費金額」是一個連續的數值，預測這類數值的問題屬於迴歸分析的範疇。'
    },
    {
        question: '在關聯規則分析中，哪個指標最能衡量規則的「有趣」程度或「驚喜」程度？',
        options: {
          A: '支持度 (Support)',
          B: '信賴度 (Confidence)',
          C: '提升度 (Lift)',
          D: '規則長度'
        },
        correctAnswer: 'C',
        explanation: '提升度(Lift)衡量了購買A對購買B的機率有多大的提升。Lift > 1表示正相關，且值越大，表示這個關聯規則越出乎意料，越「有趣」。'
    },
    {
        question: 'K-Means演算法的一個主要缺點是什麼？',
        options: {
          A: '它只能處理類別型數據。',
          B: '它需要預先指定簇的數量(K)，且對初始中心敏感。',
          C: '它的計算速度非常慢。',
          D: '它需要有標籤的數據。'
        },
        correctAnswer: 'B',
        explanation: 'K-Means的兩個主要挑戰是：如何確定最佳的K值，以及其結果可能因初始點的隨機選擇而陷入局部最優解。'
    },
    {
        question: '下列哪個演算法在處理表格數據的分類和迴歸問題上，通常表現最好？',
        options: {
          A: 'K-均值聚類',
          B: 'Apriori演算法',
          C: '梯度提升機 (如XGBoost, LightGBM)',
          D: '線性迴歸'
        },
        correctAnswer: 'C',
        explanation: '梯度提升機（尤其是XGBoost和LightGBM）是集成學習中的佼佼者，它們透過迭代地修正前一個模型的錯誤，能夠學習到非常複雜的非線性關係，在各種數據科學競賽和實際應用中，通常是處理表格數據的首選。'
    },
    {
        question: '判斷一封電子郵件是否為「垃圾郵件」，這是一個典型的什麼問題？',
        options: {
          A: '迴歸問題',
          B: '分類問題',
          C: '聚類問題',
          D: '關聯規則問題'
        },
        correctAnswer: 'B',
        explanation: '這個問題的目標是將一個數據點（郵件）分配到一個預先定義好的離散類別中（「垃圾郵件」或「非垃圾郵件」），因此是一個典型的分類問題。'
    },
    {
        question: 'DBSCAN是一種聚類演算法，相比於K-Means，其主要優勢是什麼？',
        options: {
          A: '計算速度更快。',
          B: '能發現任意形狀的簇，且能自動識別出噪聲點。',
          C: '不需要設定任何參數。',
          D: '只能處理二維數據。'
        },
        correctAnswer: 'B',
        explanation: 'DBSCAN是基於密度的聚類演算法，它不受簇必須是球狀的假設限制，能發現更複雜的形狀，並且能將不屬於任何簇的點標記為噪聲，這是K-Means做不到的。'
    },
    {
        question: '下列哪個演算法最適合用來進行用戶分群(RFM)？',
        options: {
          A: '隨機森林',
          B: '線性迴歸',
          C: '聚類分析 (如K-Means)',
          D: '關聯規則分析'
        },
        correctAnswer: 'C',
        explanation: '用戶分群的目標是將行為相似的用戶自動地分到同一組，數據本身沒有預先的「群組」標籤，這是一個典型的非監督式聚類問題。'
    },
    {
        question: '分類分析和迴歸分析都屬於監督式學習，它們最主要的區別是什麼？',
        options: {
          A: '分類需要更多數據。',
          B: '迴歸的準確率更高。',
          C: '分類預測的是離散的類別，而迴歸預測的是連續的數值。',
          D: '分類使用決策樹，迴歸使用線性模型。'
        },
        correctAnswer: 'C',
        explanation: '這是區分兩者最根本的標準：預測目標(Y)的類型。如果是類別（如「貓」、「狗」），就是分類；如果是數值（如價格、溫度），就是迴歸。'
    }
  ],
  L22303: [
    {
        question: '若要展示單一數值變數（如用戶年齡）的頻率分佈情況，最適合使用哪種圖表？',
        options: {
            A: '折線圖 (Line Chart)',
            B: '散佈圖 (Scatter Plot)',
            C: '直方圖 (Histogram)',
            D: '圓餅圖 (Pie Chart)'
        },
        correctAnswer: 'C',
        explanation: '直方圖是觀察單一數值變數分佈形狀、集中趨勢和離散程度的最佳工具。'
    },
    {
        question: '由Edward Tufte提出的「數據墨水比 (Data-Ink Ratio)」設計原則，其核心思想是什麼？',
        options: {
            A: '圖表中使用的顏色越多越好。',
            B: '盡可能地刪除圖表中非必要的裝飾性元素，讓數據成為視覺焦點。',
            C: '圖表的標題和標籤應該用最華麗的字體。',
            D: '3D效果能讓圖表更具說服力。'
        },
        correctAnswer: 'B',
        explanation: '這個原則強調簡潔和清晰，主張一個好的圖表應該將最多的「墨水」用於呈現數據本身，而不是無關的背景、網格線、特效等「圖表垃圾」。'
    },
    {
        question: '下列哪個商業智慧(BI)工具與Google生態系統（如GA, BigQuery）的整合最為無縫？',
        options: {
            A: 'Tableau',
            B: 'Microsoft Power BI',
            C: 'Google Looker Studio',
            D: 'Qlik Sense'
        },
        correctAnswer: 'C',
        explanation: 'Google Looker Studio (原Data Studio) 是Google自家的BI產品，因此它與Google Analytics, Google Ads, BigQuery, Google Sheets等其他Google服務的數據連接器最為完善和易用。'
    },
    {
        question: '若要比較不同產品線在過去一年的每月銷售額趨勢，最適合使用哪種圖表？',
        options: {
          A: '圓餅圖',
          B: '多條折線圖',
          C: '散佈圖',
          D: '盒狀圖'
        },
        correctAnswer: 'B',
        explanation: '折線圖最適合展示數據隨時間變化的趨勢。使用多條不同顏色的折線，可以在同一張圖上清晰地比較不同類別（產品線）的趨勢。'
    },
    {
        question: '在製作長條圖時，為何通常強調縱座標軸必須從0開始？',
        options: {
          A: '因為這樣比較美觀。',
          B: '因為不從0開始，會極大地誇大不同類別之間的差異，產生誤導。',
          C: '因為所有軟體的預設值都是從0開始。',
          D: '因為座標軸不能是負數。'
        },
        correctAnswer: 'B',
        explanation: '長條圖是透過長條的「長度」來比較量值。如果座標軸不從0開始，長度的比例就會被扭曲，從而誤導觀看者對數據差異的判斷。'
    },
    {
        question: 'Tableau 和 Microsoft Power BI 是什麼類型的工具？',
        options: {
          A: '資料庫管理系統',
          B: '程式開發環境 (IDE)',
          C: '商業智慧 (BI) 與數據可視化平台',
          D: '專案管理軟體'
        },
        correctAnswer: 'C',
        explanation: '這些是市場上主流的BI工具，它們讓使用者（包括非技術人員）能夠透過拖拉拽的方式，連接數據源並創建交互式的儀表板和報表。'
    },
    {
        question: '若要展示各個銷售區域佔公司總營收的比例構成，下列哪個圖表是「不」建議使用的，尤其是當區域數量超過5個時？',
        options: {
          A: '長條圖',
          B: '圓餅圖',
          C: '表格',
          D: '樹狀圖'
        },
        correctAnswer: 'B',
        explanation: '圓餅圖不適合展示過多類別的佔比，因為當扇區過多時，人眼很難準確地比較它們之間的大小差異。在這種情況下，長條圖是更好的選擇。'
    },
    {
        question: '氣泡圖 (Bubble Chart) 是散佈圖的延伸，它額外使用氣泡的什麼屬性來表示第三個數值變數？',
        options: {
          A: '顏色',
          B: '形狀',
          C: '大小',
          D: '標籤'
        },
        correctAnswer: 'C',
        explanation: '氣泡圖使用X軸、Y軸和氣泡大小，來同時展示三個數值變數之間的關係。'
    },
    {
        question: '一個有效的儀表板 (Dashboard) 設計，應遵循什麼原則？',
        options: {
          A: '盡可能地放入最多的圖表和指標。',
          B: '使用最鮮豔、最複雜的顏色搭配。',
          C: '應有明確的層次和焦點，遵循「少即是多」的原則，避免資訊過載。',
          D: '應該是靜態的，不能有任何交互功能。'
        },
        correctAnswer: 'C',
        explanation: '一個好的儀表板應該像一個好故事，有清晰的主題和結構，將最重要的資訊放在最顯眼的位置，並引導使用者逐步深入探索，而不是將所有資訊雜亂無章地堆砌在一起。'
    },
    {
        question: '下列何者是數據可視化的最終目的？',
        options: {
          A: '製作出最漂亮的圖表。',
          B: '將複雜的數據洞見，以直觀、易懂的方式傳達給決策者，以驅動行動。',
          C: '取代所有的文字報告。',
          D: '展示分析師的技術能力。'
        },
        correctAnswer: 'B',
        explanation: '數據可視化是數據分析的「最後一哩路」，其核心價值在於溝通。如果一個圖表不能清晰地傳達資訊並幫助他人做出更好的決策，那麼它就是失敗的，無論多麼美觀。'
    }
  ],
  L22401: [
    {
        question: '大數據與機器學習的關係，下列敘述何者最為貼切？',
        options: {
            A: '兩者是相互競爭的技術。',
            B: '大數據為機器學習提供了豐富的「燃料」，使其能學習到更複雜的模式。',
            C: '機器學習主要用於處理小數據，大數據需要其他技術。',
            D: '有了大數據就不再需要機器學習。'
        },
        correctAnswer: 'B',
        explanation: '大數據和機器學習是共生關係。大數據的規模和多樣性是現代機器學習（尤其是深度學習）模型能夠取得成功的關鍵前提，而機器學習是從大數據中提取價值的核心引擎。'
    },
    {
        question: '在分散式機器學習中，「數據並行 (Data Parallelism)」指的是什麼？',
        options: {
            A: '將一個巨大的模型切分到多台機器上。',
            B: '將數據集切分成多份，在多台機器上用同一個模型的複製品並行訓練。',
            C: '同時使用多種不同的演算法進行訓練。',
            D: '使用CPU和GPU並行進行計算。'
        },
        correctAnswer: 'B',
        explanation: '數據並行是最常見的分散式訓練策略。它透過將數據分發到不同節點，來加速對整個數據集的處理，每個節點上都有一個完整的模型複製品。'
    },
    {
        question: '為何說「數據的品質和數量決定了模型性能的天花板」？',
        options: {
          A: '因為演算法的選擇不重要。',
          B: '因為模型能學到的模式，不可能超出數據本身所蘊含的資訊。',
          C: '因為數據越多，計算成本越高。',
          D: '因為數據的儲存很困難。'
        },
        correctAnswer: 'B',
        explanation: '模型是從數據中學習的，數據的品質和多樣性直接限制了模型所能達到的最佳性能。如果數據本身就有問題或資訊量不足，再好的演算法也無能為力。'
    },
    {
        question: '深度學習的興起，如何在一定程度上改變了特徵工程的實踐？',
        options: {
          A: '讓手動特徵工程變得更加重要。',
          B: '深度學習模型能夠自動從原始數據中學習有效的層次化特徵表示。',
          C: '深度學習完全不需要任何特徵工程。',
          D: '深度學習只能處理手動設計好的特徵。'
        },
        correctAnswer: 'B',
        explanation: '這被稱為「表示學習」(Representation Learning)。例如，CNN可以自動從圖像像素中學習邊緣、紋理、直至物體部件的特徵，這減輕了在非結構化數據上手動設計特徵的負擔。'
    },
    {
        question: '下列哪個應用最能體現「大數據 + 機器學習」的威力？',
        options: {
          A: '在一個100人的小型公司內部，分析員工滿意度問卷。',
          B: '根據過去一週的天氣數據，預測明天的氣溫。',
          C: '在一個擁有數億用戶的電商平台，建立個人化推薦系統。',
          D: '用Excel計算一個班級的平均成績。'
        },
        correctAnswer: 'C',
        explanation: '個人化推薦系統需要從數十億甚至數百億次的用戶行為中，學習極其複雜和細微的偏好模式，這是典型的大數據機器學習問題。'
    },
    {
        question: '當一個機器學習模型本身大到無法裝入單個機器的記憶體時，應採用哪種分散式訓練策略？',
        options: {
          A: '數據並行 (Data Parallelism)',
          B: '模型並行 (Model Parallelism)',
          C: '串行訓練 (Serial Training)',
          D: '梯度累積 (Gradient Accumulation)'
        },
        correctAnswer: 'B',
        explanation: '模型並行專門解決模型過大的問題，它將模型的不同部分（如神經網路的不同層）切分到不同的計算設備上，協同完成訓練。'
    },
    {
        question: '大數據的引入，對機器學習模型的可解釋性帶來了什麼挑戰？',
        options: {
          A: '讓模型變得更容易解釋。',
          B: '在大數據上訓練的複雜模型（如深度學習）通常是黑箱，其決策過程難以解釋。',
          C: '可解釋性與數據量無關。',
          D: '大數據讓所有模型都變成了白箱。'
        },
        correctAnswer: 'B',
        explanation: '為了從大數據中學習複雜模式，我們通常需要使用更複雜的模型。而模型的複雜度與可解釋性通常成反比，這使得理解和信任模型的決策變得更加困難。'
    },
    {
        question: 'Apache Spark MLlib 是一個什麼樣的工具？',
        options: {
          A: '一個單機的機器學習函式庫，類似Scikit-learn。',
          B: '一個專為大規模數據集設計的分散式機器學習函式庫。',
          C: '一個深度學習框架，類似TensorFlow。',
          D: '一個數據可視化工具。'
        },
        correctAnswer: 'B',
        explanation: 'MLlib建立在Spark分散式計算引擎之上，提供了常用機器學習演算法的分散式版本，使其能夠在TB甚至PB級的數據上進行模型訓練。'
    },
    {
        question: '大型語言模型（如GPT-3, BERT）的成功，最關鍵的因素是什麼？',
        options: {
          A: '一個全新的演算法。',
          B: '在網際網路級別的文本大數據上進行預訓練。',
          C: '更高效的硬體。',
          D: '更高品質的人工標註。'
        },
        correctAnswer: 'B',
        explanation: '雖然演算法和硬體也很重要，但這些模型取得突破性進展的根本原因，是它們能夠從前所未有規模的文本大數據中學習到豐富的語言知識和世界模式。'
    },
    {
        question: '大數據與機器學習結合所面臨的主要挑戰「不」包含下列何者？',
        options: {
          A: '高昂的計算成本。',
          B: '數據品質與噪聲問題。',
          C: '模型的可解釋性。',
          D: '缺乏足夠的演算法。'
        },
        correctAnswer: 'D',
        explanation: '目前機器學習領域的演算法非常豐富，挑戰不在於缺乏演算法，而在於如何應對由大數據和複雜模型帶來的計算、數據品質、偏見和可解釋性等一系列新問題。'
    }
  ],
  L22402: [
    {
        question: '鑑別式AI (Discriminative AI) 的核心學習目標是什麼？',
        options: {
            A: '學習數據的潛在分佈 P(X)。',
            B: '直接學習不同類別之間的決策邊界，即條件機率 P(Y|X)。',
            C: '生成與訓練數據相似的全新數據。',
            D: '將數據分為不同的群組。'
        },
        correctAnswer: 'B',
        explanation: '鑑別式模型不關心數據是如何生成的，它只專注於一個目標：給定一個輸入X，如何最好地判斷它屬於哪個類別Y。'
    },
    {
        question: '下列何者是「大數據在鑑別式AI中的應用」的典型例子？',
        options: {
            A: '使用ChatGPT撰寫一篇文章。',
            B: '利用數百萬張已標註的病理切片，訓練一個癌症影像診斷模型。',
            C: '使用Midjourney生成一張圖片。',
            D: '發現超市購物數據中「啤酒與尿布」的關聯。'
        },
        correctAnswer: 'B',
        explanation: '癌症影像診斷是一個典型的分類問題（鑑別任務），其高準確率依賴於在海量的、帶有專家標註的圖像大數據上進行訓練。A和C是生成式AI，D是關聯規則分析。'
    },
    {
        question: '為何說大數據能夠增強鑑別式模型的「泛化能力」？',
        options: {
          A: '因為大數據能讓模型在訓練集上達到100%的準確率。',
          B: '因為更多、更多樣化的數據能更好地覆蓋真實世界的數據分佈，減少模型過擬合的風險。',
          C: '因為大數據的計算速度更快。',
          D: '因為大數據中沒有任何噪聲。'
        },
        correctAnswer: 'B',
        explanation: '泛化能力指的是模型在未見過的新數據上的表現。大數據讓模型見過「世面」，學習到的模式更具普適性，而不會過度擬合訓練數據中偶然的噪聲和特例。'
    },
    {
        question: '下列哪個演算法「不」屬於鑑別式模型？',
        options: {
          A: '邏輯迴歸 (Logistic Regression)',
          B: '支援向量機 (SVM)',
          C: '生成對抗網路 (GANs)',
          D: '決策樹 (Decision Tree)'
        },
        correctAnswer: 'C',
        explanation: 'GANs的核心目標是學習數據分佈並生成新數據，屬於生成式模型。A、B、D都是典型的監督式分類演算法，屬於鑑別式模型。'
    },
    {
        question: '在處理不平衡數據（如詐欺偵測）時，大數據的主要優勢是什麼？',
        options: {
          A: '它可以讓正負樣本的比例完全相等。',
          B: '它可以使得我們即使在極端不平衡的情況下，也能夠收集到足夠多的稀有類別（正樣本）來進行有效學習。',
          C: '它可以讓模型完全忽略稀有類別。',
          D: '它可以降低數據標註的成本。'
        },
        correctAnswer: 'B',
        explanation: '在詐欺偵測中，詐欺樣本可能只佔萬分之一。在小數據集中，可能根本沒有足夠的詐欺樣本來讓模型學習。而大數據的規模，保證了即使比例很低，稀有類別的絕對數量也足以支持模型訓練。'
    },
    {
        question: '鑑別式AI的主要挑戰或限制是什麼？',
        options: {
          A: '它無法處理圖像數據。',
          B: '它的性能高度依賴於大量、高質量的「標籤」數據。',
          C: '它無法進行預測。',
          D: '它的模型都非常簡單。'
        },
        correctAnswer: 'B',
        explanation: '鑑別式AI屬於監督式學習的範疇，其前提是擁有帶有「正確答案」（標籤）的訓練數據。而獲取和標註這些數據，通常是專案中最昂貴和耗時的部分。'
    },
    {
        question: '一個自動駕駛汽車的感知系統，需要即時偵測和分類道路上的行人、車輛等物體。這個核心任務屬於？',
        options: {
          A: '生成式AI',
          B: '鑑別式AI',
          C: '強化學習',
          D: '非監督式學習'
        },
        correctAnswer: 'B',
        explanation: '物體偵測是一個複雜的分類和定位問題，其本質是學習如何「區分」不同類別的物體，是鑑別式AI在電腦視覺領域的典型應用。'
    },
    {
        question: '鑑別式模型學習的是 P(Y|X)，其中X和Y分別代表什麼？',
        options: {
          A: 'X是模型，Y是數據。',
          B: 'X是輸入特徵，Y是目標類別標籤。',
          C: 'X是時間，Y是數值。',
          D: 'X是原因，Y是結果。'
        },
        correctAnswer: 'B',
        explanation: '在監督式學習的術語中，X代表我們用來做預測的依據（特徵），Y代表我們想要預測的目標（標籤）。P(Y|X)就是給定X，預測Y的機率。'
    },
    {
        question: '在人臉辨識應用中，使用包含各種光照、角度、表情的大數據進行訓練，主要是為了提升模型的什麼能力？',
        options: {
          A: '訓練速度',
          B: '可解釋性',
          C: '穩健性 (Robustness)',
          D: '安全性'
        },
        correctAnswer: 'C',
        explanation: '穩健性指的是模型在面對輸入數據的各種變化時，仍能保持其性能的穩定。在多樣化的大數據上訓練，能讓模型學會忽略這些無關的變化，專注於識別身份的核心特徵。'
    },
    {
        question: '下列何者不是大數據為鑑別式AI帶來的好處？',
        options: {
          A: '支持模型學習更複雜的決策邊界。',
          B: '增強模型的泛化能力。',
          C: '讓模型完全不需要人工標註的數據。',
          D: '能更好地處理長尾類別。'
        },
        correctAnswer: 'C',
        explanation: '鑑別式AI的根本前提就是需要標籤數據。大數據只是意味著我們需要更多、更多樣化的標籤數據，而不能免除標註的需求。'
    }
  ],
  L22403: [
    {
        question: '生成式AI (Generative AI) 的核心學習目標是什麼？',
        options: {
            A: '學習數據的潛在分佈 P(X)，以便能生成新數據。',
            B: '學習如何將數據點準確分類。',
            C: '預測一個連續的數值。',
            D: '從數據中移除噪聲。'
        },
        correctAnswer: 'A',
        explanation: '生成式模型的核心是理解數據的「內在規律」和「結構」，即學習其機率分佈P(X)。一旦學會了這個分佈，就能從中進行採樣，創造出全新的、符合該分佈的數據。'
    },
    {
        question: '為何網際網路級別的大數據對於訓練大型語言模型 (LLMs) 至關重要？',
        options: {
            A: '因為大數據的下載速度比較快。',
            B: '因為只有海量、多樣化的數據才能讓模型學會語言的複雜模式和世界知識。',
            C: '因為大數據中不包含任何個人隱私。',
            D: '因為法律規定必須使用大數據。'
        },
        correctAnswer: 'B',
        explanation: '人類語言的複雜性和其背後的世界知識是極其龐大的。只有讓模型「閱讀」網際網路規模的文本，它才能學到足夠的模式，從而具備強大的理解和生成能力。'
    },
    {
        question: '在訓練數據稀少的場景下（如醫療影像），如何利用生成式AI來幫助下游的鑑別式模型？',
        options: {
          A: '用生成式AI來取代醫生。',
          B: '用生成式AI來生成大量新的、人工的、但高度逼真的影像數據，以擴充訓練集。',
          C: '用生成-式AI來設計新的醫院。',
          D: '生成式AI對此沒有幫助。'
        },
        correctAnswer: 'B',
        explanation: '這被稱為「數據增強」(Data Augmentation)，是生成式AI輔助鑑別式AI的一個典型應用。它能有效緩解因數據不足導致的過擬合問題，提升鑑別模型的性能。'
    },
    {
        question: '下列哪個應用不屬於生成式AI的範疇？',
        options: {
          A: '文生圖模型 (Text-to-Image)',
          B: '大型語言模型 (LLMs)',
          C: '藥物分子生成',
          D: '信用卡詐欺偵測'
        },
        correctAnswer: 'D',
        explanation: '信用卡詐欺偵測是一個典型的二元分類問題，其目標是「判斷」一筆交易是否為詐欺，屬於鑑別式AI。A、B、C的目標都是「創造」全新的內容。'
    },
    {
        question: '生成式AI學習的是數據的 P(X)，其中X代表什麼？',
        options: {
          A: '模型的參數',
          B: '數據的標籤',
          C: '數據本身',
          D: '時間變數'
        },
        correctAnswer: 'C',
        explanation: '在生成式模型的語境下，P(X)代表數據X在真實世界中出現的機率分佈。模型試圖學習這個分佈，以便能生成符合該分佈的新樣本。'
    },
    {
        question: '生成式對抗網路(GAN)在訓練時，如果生成器產生的內容單調、重複，只能生成幾種有限的樣本，這個問題被稱為什麼？',
        options: {
          A: '過擬合 (Overfitting)',
          B: '模式崩潰 (Mode Collapse)',
          C: '幻覺 (Hallucination)',
          D: '梯度消失 (Vanishing Gradient)'
        },
        correctAnswer: 'B',
        explanation: '模式崩潰是GAN訓練中常見的一個難題，指的是生成器找到了幾個能輕易騙過判別器的「捷徑」，而不再去探索數據的完整分佈，導致生成結果缺乏多樣性。'
    },
    {
        question: '大型生成模型（如GPT-3）能實現零樣本/少樣本學習，其根本原因是什麼？',
        options: {
          A: '它們的模型結構非常簡單。',
          B: '它們內建了所有任務的規則。',
          C: '它們透過在超大規模數據集上預訓練，學會了通用的模式和概念關聯，具備了強大的泛化能力。',
          D: '它們的推論速度非常快。'
        },
        correctAnswer: 'C',
        explanation: '零樣本/少樣本能力的基礎，是模型在預訓練階段從海量數據中學到的「世界知識」。模型能夠將新任務指令，與其已有的知識庫進行關聯，從而理解並執行任務，而無需針對性的訓練。'
    },
    {
        question: '下列何者是訓練頂尖生成式AI模型面臨的主要挑戰？',
        options: {
          A: '巨大的計算成本。',
          B: '缺乏足夠的演算法。',
          C: '很難找到應用場景。',
          D: '模型通常過於簡單。'
        },
        correctAnswer: 'A',
        explanation: '訓練像GPT-4這樣的大型基礎模型，需要數千顆GPU持續運行數月，其對計算資源、電力和資金的消耗都是天文數字，是只有少數幾家公司能承擔的。'
    },
    {
        question: '如何客觀、量化地評估生成內容的「品質」或「創造力」，為何是一個困難的問題？',
        options: {
          A: '因為沒有足夠的工具來進行評估。',
          B: '因為這些概念本身是主觀的，不像分類模型的準確率那樣有明確的對錯標準。',
          C: '因為生成模型的輸出結果都是一樣的。',
          D: '因為評估過程太耗時。'
        },
        correctAnswer: 'B',
        explanation: '評估生成模型的輸出品質，往往需要依賴人類的主觀判斷或設計複雜的代理指標，這比評估鑑別式模型的準確率要困難得多。'
    },
    {
        question: 'Midjourney, Stable Diffusion 等文生圖模型的驚豔效果，主要歸功於什麼？',
        options: {
          A: '使用了最新的電腦硬體。',
          B: '在數十億個「圖像-文本描述」對的大數據集上進行訓練。',
          C: '藝術家的手動調整。',
          D: '更快的網路速度。'
        },
        correctAnswer: 'B',
        explanation: '這些模型的核心能力，來自於從海量的圖文對數據中，學習到了文字概念與視覺元素之間的精確映射關係。沒有這樣規模的數據，就不可能有如此驚人的效果。'
    }
  ],
  L22404: [
    {
        question: '下列哪項技術，透過在數據查詢或模型訓練中加入經過計算的噪聲，來提供數學上可證明的個人隱私保護？',
        options: {
            A: '數據加密 (Encryption)',
            B: '聯邦學習 (Federated Learning)',
            C: '差分隱私 (Differential Privacy)',
            D: 'K-匿名 (K-Anonymity)'
        },
        correctAnswer: 'C',
        explanation: '差分隱私是當前隱私保護領域的黃金標準，它透過對結果添加噪聲，使得任何單個用戶的數據對最終結果的影響都微乎其微，從而保護了個體資訊。'
    },
    {
        question: '歐盟的通用數據保護條例 (GDPR) 強調的核心原則不包含下列何者？',
        options: {
            A: '告知同意',
            B: '數據最小化',
            C: '數據最大化，即收集盡可能多的數據。',
            D: '目的限制'
        },
        correctAnswer: 'C',
        explanation: 'GDPR強調的是「數據最小化原則」，即企業只應收集為了達成特定、合法目的所必需的最少量數據，與「數據最大化」的理念完全相反。'
    },
    {
        question: '「數據保留在本地，只有模型的更新被加密後傳回中央伺服器進行聚合」，這描述了哪種隱私保護技術？',
        options: {
            A: '差分隱私',
            B: '聯邦學習',
            C: '數據加密',
            D: '數據去識別化'
        },
        correctAnswer: 'B',
        explanation: '聯邦學習是一種分散式的機器學習方法，它的核心優勢在於，原始的、可能包含敏感資訊的數據永遠不需要離開用戶的設備或本地伺服器，從而極大地保護了數據隱私。'
    },
    {
        question: '即使數據集中的姓名、身分證號等直接標識符已被移除，攻擊者仍可能透過關聯多個看似無關的匿名化數據點，來重新識別出個人身份。這種攻擊稱為什麼？',
        options: {
          A: '數據中毒攻擊',
          B: '鏈接攻擊',
          C: '對抗性攻擊',
          D: '阻斷服務攻擊'
        },
        correctAnswer: 'B',
        explanation: '鏈接攻擊凸顯了簡單匿名化的局限性。攻擊者可以將匿名化的數據集與其他公開可用的數據集進行關聯（鏈接），從而推斷出個人的真實身份。'
    },
    {
        question: '在AI的合規審計中，銀行使用LIME或SHAP等工具為每一個拒貸決策提供解釋報告，這主要是為了滿足什麼要求？',
        options: {
          A: '數據安全要求',
          B: '模型性能要求',
          C: '可解釋性與反對自動化決策權的要求',
          D: '數據加密要求'
        },
        correctAnswer: 'C',
        explanation: 'GDPR等法規保障了用戶在面對對其有重大影響的自動化決策時的知情權。提供可解釋性報告，是讓決策過程透明化、可被審計和申訴的關鍵。'
    },
    {
        question: '下列何者是「隱私保護設計 (Privacy by Design)」的核心理念？',
        options: {
          A: '在系統開發完成後，再添加隱私保護功能。',
          B: '將隱私保護作為系統設計的預設核心要求，而非事後補救。',
          C: '只在用戶投訴時才考慮隱私問題。',
          D: '隱私保護是法務部門的責任，與開發無關。'
        },
        correctAnswer: 'B',
        explanation: '「由設計而始」的理念強調，應在AI生命週期的每個階段，都主動地、系統性地考慮和嵌入隱私保護措施，而不是將其視為一個附加項。'
    },
    {
        question: '實施「最小權限原則」是為了防範哪一類安全風險？',
        options: {
          A: '外部駭客的網路攻擊。',
          B: '因內部人員疏失或帳號被盜導致的未授權訪問。',
          C: '數據在傳輸過程中被竊聽。',
          D: 'AI模型的對抗性攻擊。'
        },
        correctAnswer: 'B',
        explanation: '最小權限原則透過確保每個用戶只能訪問其工作所必需的最少量數據，來限制單點故障（如某個員工帳號被盜）可能造成的損害範圍。'
    },
    {
        question: '差分隱私和聯邦學習，這兩種技術在隱私與效用之間存在什麼樣的權衡？',
        options: {
          A: '它們能同時最大化隱私和模型效用。',
          B: '過於強力的隱私保護措施（如加入過多噪聲）可能會降低數據的可用性和分析模型的準確性。',
          C: '它們主要提升模型效用，對隱私保護沒有幫助。',
          D: '它們的隱私保護效果與模型效用完全無關。'
        },
        correctAnswer: 'B',
        explanation: '這是一個典型的權衡關係。例如，在差分隱私中，加入的噪聲越多，隱私保護程度越高，但數據的信號也會被淹沒得越嚴重，導致模型準確率下降。需要在兩者之間找到可接受的平衡。'
    },
    {
        question: '對抗性訓練 (Adversarial Training) 是為了解決AI的哪種特定安全威脅？',
        options: {
          A: '數據中毒',
          B: '模型竊取',
          C: '對抗性攻擊',
          D: '提示注入'
        },
        correctAnswer: 'C',
        explanation: '對抗性訓練的核心思想是，在訓練過程中，主動地生成對抗性樣本並將其加入訓練集，讓模型學會如何應對這些惡意擾動，從而提升其穩健性。'
    },
    {
        question: '下列哪項不是GDPR賦予用戶的權利？',
        options: {
          A: '被遺忘權',
          B: '數據可攜權',
          C: '反對自動化決策權',
          D: '免費獲取AI模型原始碼權'
        },
        correctAnswer: 'D',
        explanation: 'GDPR保障了用戶對其「個人數據」的控制權，但並不包含要求企業提供涉及其商業機密的AI模型原始碼的權利。'
    }
  ]
};
